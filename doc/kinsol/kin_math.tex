%===================================================================================
\chapter{Mathematical Considerations}\label{s:math}
%===================================================================================

{\kinsol} solves nonlinear algebraic systems in real $N$-space.

Using Newton's method, or the Picard iteration, one can solve
\index{nonlinear system!definition}
\begin{equation}\label{nonlinear_system}
  F(u) = 0 \, , \quad F:\mbox{\bf R}^N \rightarrow \mbox{\bf R}^N \, ,
\end{equation}
given an initial guess $u_0$.
Using a fixed-point iteration, the convergence of which can be improved with Anderson
acceleration, one can solve
\index{fixed-point system!definition}
\begin{equation}\label{fixed-point_system}
  G(u) = u \, , \quad G:\mbox{\bf R}^N \rightarrow \mbox{\bf R}^N \, ,
\end{equation}
given an initial guess $u_0$.


%% NEWTON ITERATION

\subsubsection*{Basic Newton iteration}
%%
Depending on the linear solver used, {\kinsol} can employ either an Inexact Newton
method~\cite{Bro:87,BrSa:90,DES:82,DeSc:96,Kel:95}, or a Modified Newton method.
At the highest level, {\kinsol} implements the following iteration scheme:

\vspace{1ex}
\index{Inexact Newton iteration!definition}
\index{Modified Newton iteration!definition}
\begin{enumerate}
   \item Set $u_0 = $ an initial guess
   \item For $n = 0, 1, 2,...$ until convergence do:
      \begin{enumerate}
          \item Solve $J(u_n)\delta_n = -F(u_n)$\label{e:Newton}
          \item Set $u_{n+1} = u_n + \lambda \delta_n$,
          $0 < \lambda \leq 1$
          \item Test for convergence
      \end{enumerate}
\end{enumerate}
Here, $u_n$ is the $n$th iterate to $u$, and $J(u) = F'(u)$ is the system
Jacobian. At each stage in the iteration process, a scalar multiple of the
step $\delta_n$, is added to $u_n$ to produce a new iterate, $u_{n+1}$.
A test for convergence is made before the iteration continues.


%% NEWTON METHODS

\subsubsection*{Newton method variants}
%%
For solving the linear system given in step (\ref{e:Newton}), {\kinsol} provides
several choices, including the option of a user-supplied
linear solver module. The linear solver modules distributed with {\sundials}
are organized in two families, a {\em direct} family comprising direct linear
solvers for dense, banded, or sparse matrices and a {\em spils}
family comprising scaled preconditioned
iterative (Krylov) linear solvers.
%%
The methods offered through these modules are as follows:
\begin{itemize}
\item dense direct solvers, using either an internal implementation or
  a BLAS/LAPACK implementation (serial or threaded vector modules only),
\item band direct solvers, using either an internal implementation or
  a BLAS/LAPACK implementation (serial or threaded vector modules only),
\item sparse direct solver interfaces, using either the KLU sparse solver
  library \cite{DaPa:10,KLU_site}, or the thread-enabled SuperLU\_MT sparse
  solver library \cite{Li:05,DGL:99,SuperLUMT_site} (serial or threaded
  vector modules only) [Note that users will need to download and install the
  {\klu} or {\superlumt} packages independent of {\kinsol}],
\item {\spgmr}, a scaled preconditioned GMRES (Generalized Minimal Residual method)
  solver,
\item {\spfgmr}, a scaled preconditioned FGMRES (Flexible Generalized
  Minimal Residual method) solver,
\item {\spbcg}, a scaled preconditioned Bi-CGStab (Bi-Conjugate Gradient Stable
  method) solver,
\item {\sptfqmr}, a scaled preconditioned TFQMR (Transpose-Free Quasi-Minimal
  Residual method) solver, or
\item {\pcg}, a scaled preconditioned CG (Conjugate Gradient method) solver.
\end{itemize}
When using a direct linear solver, the linear system in 2(a) is
solved exactly, thus resulting in a Modified Newton method (the Jacobian matrix
is normally out of date; see below\footnote{{\kinsol} allows the user to enforce
a Jacobian evaluation at each iteration thus allowing for an Exact Newton iteration.}).
%%
Note that the dense, band, and sparse direct linear solvers can only
be used with the serial and threaded vector representations.

When using an iterative linear solver, the linear system in (\ref{e:Newton}) is
solved only approximately, thus resulting in an Inexact Newton method. Here
right preconditioning is available by way of the preconditioning setup and solve
routines supplied by the user, in which case the iterative method is applied to
the linear systems $(JP^{-1})(P\delta) = -F$, where $P$ denotes the right
preconditioning matrix.

Additionally, it is possible for users to supply a matrix-based
iterative linear solver to {\kinsol}, resulting in a Modified Inexact
Newton method.  As with the direct linear solvers, the Jacobian matrix
is updated infrequently; similarly as with iterative linear solvers
the linear system is solved only approximately.

%% JACOBIAN UPDATE STRATEGY

\subsubsection*{Jacobian information update strategy}
\label{ss:JacUpdate}
%%
In general, unless specified otherwise by the user, {\kinsol} strives to update
Jacobian information (the actual system Jacobian $J$ in the case of matrix-based linear
solvers, and the preconditioner matrix $P$ in the case of iterative linear solvers)
as infrequently as possible to balance the high costs of matrix operations against
other costs. Specifically, these updates occur when:
\begin{itemize}
\item the problem is initialized,
\item $\|\lambda\delta_{n-1}\|_{D_u,\infty} > 1.5$ (Inexact Newton only),
\item \id{mbset}$=10$ nonlinear iterations have passed since the last update,
\item the linear solver failed recoverably with outdated Jacobian information,
\item the global strategy failed with outdated Jacobian information, or
\item $\|\lambda\delta_{n}\|_{D_u,\infty} < $ {\sc steptol} with
  outdated Jacobian or preconditioner information.
\end{itemize}
{\kinsol} allows, through optional solver inputs, changes to the above strategy.
Indeed, the user can disable the initial Jacobian information evaluation or change
the default value of \id{mbset}, the number of nonlinear iterations after which a
Jacobian information update is enforced.

%% SCALING

\subsubsection*{Scaling}
\label{ss:Scaling}
%%
To address the case of ill-conditioned nonlinear systems, {\kinsol} allows
prescribing scaling factors both for the solution vector and for the residual
vector.
%%
For scaling to be used, the user should supply values $D_u$, which are diagonal
elements of the scaling matrix such that $D_u u_n$ has all components roughly
the same magnitude when $u_n$ is close to a solution, and $D_F$, which are diagonal
scaling matrix elements such that $D_F F$ has all components roughly the same
magnitude when $u_n$ is not too close to a solution.
In the text below, we use the following scaled norms:
\begin{equation}
\|z\|_{D_u} = \|D_u z\|_2, \;\; \|z\|_{D_F} = \|D_F z\|_2, \;\;
\|z\|_{D_u,\infty} = \|D_u z\|_{\infty}, \;\; {\rm and} \;\;
\|z\|_{D_F,\infty} = \|D_F z\|_{\infty}
\end{equation}
where $\| \cdot \|_{\infty}$ is the max norm.  When scaling values are provided
for the solution vector, these values are automatically incorporated into the
calculation of the perturbations used for the default difference quotient
approximations for Jacobian information; see (\ref{e:sigmaDQ_direct}) and
(\ref{e:sigmaDQ_iterative}) below.

%% GLOBALIZATION

\subsubsection*{Globalization strategy}
%%
Two methods of applying a computed step $\delta_n$ to the
previously computed solution vector are implemented. The first and
simplest is the standard Newton strategy which applies step 2(b) as
above with $\lambda$ always set to $1$. The other method is a
global strategy, which attempts to use the direction implied by
$\delta_n$ in the most efficient way for furthering convergence of
the nonlinear problem. This technique is implemented in the second
strategy, called Linesearch.  This option employs both the
$\alpha$ and $\beta$ conditions of the Goldstein-Armijo linesearch
given in \cite{DeSc:96} for step 2(b), where $\lambda$ is chosen
to guarantee a sufficient decrease in $F$ relative to the step
length as well as a minimum step length relative to the initial
rate of decrease of $F$.  One property of the algorithm is that
the full Newton step tends to be taken close to the solution.

{\kinsol} implements a backtracking algorithm to first find
the value $\lambda$ such that $u_n + \lambda \delta_n$
satisfies the sufficient decrease condition (or $\alpha$-condition)
\[
F(u_n + \lambda\delta_n) \le F(u_n) + \alpha \nabla F(u_n)^T \lambda\delta_n \, ,
\]
where $\alpha = 10^{-4}$.
%%
Although backtracking in itself guarantees that the step is not too small,
{\kinsol} secondly relaxes $\lambda$ to satisfy the so-called $\beta$-condition
(equivalent to Wolfe's curvature condition):
\[
F(u_n + \lambda\delta_n) \ge F(u_n) + \beta \nabla F(u_n)^T \lambda\delta_n \, ,
\]
where $\beta = 0.9$. During this second phase, $\lambda$ is allowed to
vary in the interval $[\lambda_{min} , \lambda_{max}]$ where
\[
\lambda_{min} =  \frac{\mbox{\sc steptol}}{\| \bar\delta_n\|_{\infty}} \, , \quad
\bar\delta_n^j = \frac{\delta_n^j}{1/D_u^j + |u^j|} \, ,
\]
and $\lambda_{max}$ corresponds to the maximum feasible step size at the
current iteration (typically $\lambda_{max} = \mbox{\sc stepmax} / \|\delta_n\|_{D_u}$).
In the above expressions, $v^j$ denotes the $j$th component of a vector $v$.

For more details, the reader is referred to \cite{DeSc:96}.

%% STOPPING CRITERIA

\subsubsection*{Nonlinear iteration stopping criteria}
%%
Stopping criteria for the Newton method are applied to
both of the nonlinear residual and the step length.  For the
former, the Newton iteration must pass a stopping test
\[ \|F(u_n)\|_{D_F,\infty} < \mbox{\sc ftol} \, , \]
where {\sc ftol} is an input scalar tolerance with a default value
of $U^{1/3}$.  Here $U$ is the machine unit roundoff.
For the latter, the Newton method will terminate
when the maximum scaled step is below a given tolerance
\[ \|\lambda\delta_n\|_{D_u,\infty} < \mbox{\sc steptol} \, , \]
where {\sc steptol} is an input scalar tolerance with a default
value of $U^{2/3}$.  Only the first condition (small residual)
is considered a successful completion of {\kinsol}.  The second
condition (small step) may indicate that the iteration is stalled
near a point for which the residual is still unacceptable.

%% ADDITIONAL CONSTRAINTS

\subsubsection*{Additional constraints}
%%
As a user option, {\kinsol} permits the application of inequality
constraints, $u^i > 0$ and $u^i < 0$, as well as $u^i \geq 0$ and
$u^i \leq 0$, where $u^i$ is the $i$th component of $u$.  Any such
constraint, or no constraint, may be imposed on each component.
{\kinsol} will reduce step lengths in order to ensure that no
constraint is violated.  Specifically, if a new Newton iterate
will violate a constraint, the maximum step length along
the Newton direction that will satisfy all constraints is found,
and $\delta_n$ in Step 2(b) is scaled to take a step of that length.

%% RESIDUAL MONITORING FOR MATRIX-BASED JACOBIAN UPDATE

\subsubsection*{Residual monitoring for Modified Newton method}
\label{ss:ModifiedNewtonResidualMon}
%%
When using a matrix-based linear solver, in addition to the strategy
described above for the update of the Jacobian matrix, {\kinsol} also
provides an optional nonlinear residual monitoring scheme to control
when the system Jacobian is updated. Specifically, a Jacobian update
will also occur when \id{mbsetsub}$=5$ nonlinear iterations have
passed since the last update and
\[ \|F(u_n)\|_{D_F} > \omega \|F(u_m)\|_{D_F} \, , \]
where $u_n$ is the current iterate and $u_m$ is the iterate at the last Jacobian
update. The scalar $\omega$ is given by
\begin{equation}\label{resmon_omega}
\omega = \min \left (\omega_{min} \, e^{\max \left ( 0 , \rho - 1 \right )} , \omega_{max}\right ) \, ,
\end{equation}
with $\rho$ defined as
\begin{equation}
\rho = \frac{\|F(u_n) \|_{D_F}}{\mbox{{\sc ftol}}} \, ,
\end{equation}
where {\sc ftol} is the input scalar tolerance discussed before.
Optionally, a constant value $\omega_{const}$ can be used for the parameter $\omega$.

The constants controlling the nonlinear residual monitoring algorithm can
be changed from their default values through optional inputs to {\kinsol}.
These include the parameters $\omega_{min}$ and $\omega_{max}$, the constant
value $\omega_{const}$, and the threshold \id{mbsetsub}.

%% INEXACT NEWTON SPECIFIC

\subsubsection*{Stopping criteria for iterative linear solvers}
\label{ss:InexactNewtonStopCrit}
%%
When using an Inexact Newton method (i.e. when an iterative linear solver is
used), the convergence of the overall nonlinear solver is intimately coupled
with the accuracy with which the linear solver in 2(a) above is solved.
{\kinsol} provides three options for stopping criteria for the linear system
solver, including the two algorithms of Eisenstat and Walker \cite{EiWa:96}.
More precisely, the Krylov iteration must pass a stopping test
\[ \|J \delta_n + F\|_{D_F} < (\eta_n + U) \|F\|_{D_F} \, , \]
where $\eta_n$ is one of:
\begin{description}
\item[Eisenstat and Walker Choice 1]
  \[
  \eta_n = \frac{\left|\; \|F(u_n)\|_{D_F}
      - \|F(u_{n-1}) + J(u_{n-1}) \delta_n \|_{D_F}
      \; \right|}
  {\|F(u_{n-1})\|_{D_F}} \, ,
  \]
\item[Eisenstat and Walker Choice 2]
  \[
  \eta_n = \gamma
  \left( \frac{ \|F(u_n)\|_{D_F}}{\|F(u_{n-1})\|_{D_F}} \right)^{\alpha} \, ,
  \]
where default values of $\gamma$ and $\alpha$ are $0.9$ and $2$,
respectively.
\item[Constant $\eta$]
  \[
  \eta_n = constant,
  \]
with 0.1 as the default.
\end{description}
The default strategy is "Eisenstat and Walker Choice 1".
For both options 1 and 2, appropriate safeguards are incorporated to ensure that
$\eta$ does not decrease too quickly~\cite{EiWa:96}.

%% JACOBIAN APPROXIMATIONS

\subsubsection*{Difference quotient Jacobian approximations}
%%
With the dense and banded matrix-based linear solvers, the Jacobian
may be supplied by a user routine, or approximated by difference
quotients, at the user's option.  In the latter case, we use the usual
approximation
\begin{equation}\label{e:JacDQ}
  J^{ij} = [F^i(u+\sigma_j e^j) - F^i(u)]/\sigma_j \, .
\end{equation}
The increments $\sigma_j$ are given by
\begin{equation}\label{e:sigmaDQ_direct}
  \sigma_j = \sqrt{U} \; \max\left\{ |u^j| , 1/D_u^j \right\} \, .
\end{equation}
%%
In the dense case, this scheme requires $N$ evaluations of $F$,
one for each column of $J$.  In the band case, the columns of
$J$ are computed in groups, by the Curtis-Powell-Reid algorithm,
with the number of $F$ evaluations equal to the bandwidth.
The parameter $U$ above can (optionally) be replaced by a
user-specified value, \id{relfunc}.

We note that with sparse and user-supplied matrix-based linear
solvers, the Jacobian {\em must} be supplied by a user routine,
i.e. it is not approximated internally within {\kinsol}.

In the case of a matrix-free iterative linear solver, Jacobian
information is needed only as matrix-vector products $Jv$.  If a
routine for $Jv$ is not supplied, these products are approximated by
directional difference quotients as
\begin{equation}\label{e:JvDQ}
J(u) v \approx [F(u+\sigma v) - F(u)]/\sigma \, ,
\end{equation}
where $u$ is the current approximation to a root of
(\ref{nonlinear_system}), and $\sigma$ is a scalar. The choice of
$\sigma$ is taken from \cite{BrSa:90} and is given by
\begin{equation}\label{e:sigmaDQ_iterative}
  \sigma = \frac{\max \{|u^T v|, u^T_{typ} |v|\}}{\|v\|_2^2}
  \mbox{sign}(u^T v) \sqrt{U} \, ,
\end{equation}
where $u_{typ}$ is a vector of typical values for the absolute
values of the solution (and can be taken to be inverses of the
scale factors given for $u$ as described below).
This formula is suitable for {\it scaled} vectors $u$ and $v$, and
so is applied to $D_u u$ and $D_u v$.
The parameter $U$ above can (optionally) be replaced by a
user-specified value, \id{relfunc}.
Convergence of the Newton method is maintained as long as the
value of $\sigma$ remains appropriately small, as shown in~\cite{Bro:87}.

%%
%% FIXED POINT ITERATION

\subsubsection*{Basic Fixed Point iteration}
%%
The basic fixed-point iteration scheme implemented in {\kinsol} is given by:

\vspace{1ex}
\index{Fixed-point iteration!definition}
\begin{enumerate}
   \item Set $u_0 = $ an initial guess
   \item For $n = 0, 1, 2,...$ until convergence do:
      \begin{itemize}
          \item[(a)] Set $u_{n+1} = G(u_n)$.
          \item[(b)] Test for convergence.
      \end{itemize}
\end{enumerate}
Here, $u_n$ is the $n$th iterate to $u$.
At each stage in the iteration process, function $G$ is applied to the current
iterate to produce a new iterate, $u_{n+1}$.
A test for convergence is made before the iteration continues.


%% PICARD ITERATION

For Picard iteration, as implemented in {\kinsol}, we consider a special form
of the nonlinear function $F$, such that $F(u) = Lu - N(u)$, where $L$ is a constant
nonsingular matrix and $N$ is (in general) nonlinear.  Then the fixed-point
function $G$ is defined as $G(u) = u - L^{-1}F(u)$.
The Picard iteration is given by:

\vspace{1ex}
\index{Picard iteration!definition}
\begin{enumerate}
   \item Set $u_0 = $ an initial guess
   \item For $n = 0, 1, 2,...$ until convergence do:
      \begin{itemize}
          \item[(a)] Set $u_{n+1} = G(u_n) = u_n - L^{-1}F(u_n)$.
          \item[(b)] Test $F(u_{n+1})$ for convergence.
      \end{itemize}
\end{enumerate}
Here, $u_n$ is the $n$th iterate to $u$.
Within each iteration, the Picard step is computed then added to $u_n$ to produce
the new iterate.  Next, the nonlinear residual
function is evaluated at the new iterate, and convergence is checked.
Noting that $L^{-1}N(u) = u - L^{-1}F(u)$, the above iteration can be written in the same form
as a Newton iteration except that here, $L$ is in the role of the Jacobian.
Within {\kinsol}, however, we leave this in a fixed-point form as above.
For more information, see p. 182 of \cite{Ortega-Rheinbolt00}.


%% ANDERSON ACCELERATION

\subsubsection*{Anderson Acceleration}
%%
The Picard and fixed point methods can be significantly accelerated using
Anderson's method \cite{Anderson65, Walker-Ni09, Fang-Saad09, LWWY11}.
Anderson acceleration can be formulated as follows:

\vspace{1ex}
\index{Anderson acceleration!definition}
\begin{enumerate}
   \item Set $u_0 = $ an initial guess and $m \ge 1$
   \item Set $u_1 = G(u_0)$
   \item For $n = 1, 2,...$ until convergence do:
      \begin{itemize}
          \item[(a)] Set $m_n = \min\{m,n\}$
          \item[(b)] Set $F_{n} = (f_{n-m_n}, \ldots, f_n)$, where $f_i=G(u_i)-u_i$
          \item[(c)] Determine $\alpha^{(n)} = (\alpha_0^{(n)}, \ldots, \alpha_{m_n}^{(n)})$ that solves
            $\min_\alpha  \| F_n \alpha^T \|_2$ such that $\sum_{i=0}^{m_n} \alpha_i = 1$
          \item[(d)] Set $u_{n+1} = \beta \sum_{i=0}^{m_n} \alpha_i^{(n)} G(u_{n-m_n+i}) +
            (1-\beta) \sum_{i=0}^{m_n} \alpha_i^{(n)} u_{n-m_{n}+i}$
          \item[(e)] Test for convergence
      \end{itemize}
\end{enumerate}

It has been implemented in {\kinsol} by turning the constrained linear least-squares
problem in Step (c) into an unconstrained one leading to the algorithm given below:

\vspace{1ex}
\index{Anderson acceleration UA!definition}
\begin{enumerate}
   \item Set $u_0 = $ an initial guess and $m \ge 1$
   \item Set $u_1 = G(u_0)$
   \item For $n = 1, 2,...$ until convergence do:
      \begin{itemize}
          \item[(a)] Set $m_n = \min\{m,n\}$
          \item[(b)] Set $\Delta F_{n} = (\Delta f_{n-m_n}, \ldots, \Delta f_{n-1})$,
            where $\Delta f_i = f_{i+1} - f_i$ and $f_i=G(u_i)-u_i$
          \item[(c)] Determine $\gamma^{(n)} = (\gamma_0^{(n)}, \ldots, \gamma_{m_n-1}^{(n)})$
            that solves $\min_\gamma  \| f_n - \Delta F_n \gamma^T \|_2$
          \item[(d)] Set $u_{n+1} = G(u_n)-\sum_{i=0}^{m_n-1} \gamma_i^{(n)}
            \Delta g_{n-m_n+i} - (1-\beta)(f(u_n) - \sum_{i=0}^{m_n-1} \gamma_i^{(n)} \Delta f_{n-m_n+i})$ with
            $\Delta g_i = G(u_{i+1}) - G(u_i)$
          \item[(e)] Test for convergence
      \end{itemize}
\end{enumerate}

The least-squares problem in (c) is solved by applying a QR factorization to
$\Delta F_n = Q_n R_n$ and solving $R_n \gamma = Q_n^T f_n$. By default the
damping is disabled i.e., $\beta = 1.0$.


\subsubsection*{Fixed-point - Anderson Acceleration Stopping Criterion}

The default stopping criterion is
\[ \|G(u_{n+1}) - u_{n+1} \|_{D_F,\infty} < \mbox{\sc gtol} \, , \]
where $D_F$ is a user-defined diagonal matrix that can be the identity
or a scaling matrix chosen so that the components of $D_F (G(u)-u)$ have
roughly the same order of magnitude.
Note that when using Anderson acceleration,
convergence is checked after the acceleration is applied.


\subsubsection*{Picard - Anderson Acceleration Stopping Criterion}

The default stopping criterion is
\[ \|F(u_{n+1})\|_{D_F,\infty} < \mbox{\sc ftol} \, , \]
where $D_F$ is a user-defined diagonal matrix that can be the identity
or a scaling matrix chosen so that the components of $D_F F(u)$ have
roughly the same order of magnitude.
Note that when using Anderson acceleration,
convergence is checked after the acceleration is applied.
